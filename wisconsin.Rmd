---
title: "Example analysis of Wisconsin breast cancer data"
author: "Clement Lee (Literate Programming Team)"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: true
    toc: false
  html_document:
    number_sections: true
classoption: a4paper
---

# Introduction & exploratory data analysis
This is an example analysis of the Wisconsin breast cancer data (available [here](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)) done in R. Before looking at the data, we load the packages required. We also set the theme of the plots using `theme_set()` in package `ggplot2`.

```{r prelim, message = FALSE}
library(tibble)
library(dplyr)
library(readr)
library(ggplot2)
library(caret)
library(ROCR)
library(pROC)
theme_set(theme_bw(12))
knitr::opts_chunk$set(fig.align = "center")
```

## Read data
Now we read the data, available as a local csv file in the relative path (`breast-cancer-wisconsin/`) below. We use `head()` function to print the first few lines and show the structure of the data frame, `dim()` to list the dimensions, and `colnames()` (or `names()`) to list the column names. We also change the `diagnosis` variable to a factor.

```{r read}
cancer_data <- as_tibble(read.csv("breast-cancer-wisconsin/data.csv"))
head(cancer_data)
cancer_data$diagnosis <- as.factor(cancer_data$diagnosis)
dim(cancer_data)
colnames(cancer_data)
```

We use the following code to remove columns with missing values (`NA`), and use `head()` again to have a glimpse of the remaining columns.

```{r remove_na}
not_any_na <- function(x) all(!is.na(x))
cancer_data <- cancer_data |> select(where(not_any_na))
head(cancer_data)
```

## Basic summaries
We count the number of Malignant and Benign tumours in the data, using the column `diagnosis`. We print such counts in the R output:

```{r counts, results = "hold"}
paste0("Number of Malignant tumours: ", table(cancer_data$diagnosis)["M"])
paste0("Number of Benign tumours: ", table(cancer_data$diagnosis)["B"])
```

We can also plot these counts:
```{r plot_counts}
cancer_data |>
  ggplot() +
  geom_bar(aes(diagnosis, fill = diagnosis)) +
  theme_bw(12) +
  labs(title = "A count of benign and malignant tumours")
```

## Dropping variables
Here's an example on how to drop some variables and showing the dimensions of the resultant data frame:
```{r drop, results = "hold"}
input_data <- cancer_data |> select(-id, -diagnosis)
paste0("Sample size: ", nrow(input_data))
paste0("Number of independent variables: ", ncol(input_data))
```





# Visualisations

## Histogram & density
We would like to look at the distribution of three different variables according to the tumour status. As the same kind of plot is required, a function is written for convenience.

```{r plot_hist, fig.show = "hold"}
plot_hist_den <- function(df = cancer_data, var, label) {
  df |>
    ggplot() +
    geom_histogram(
      aes({{ var }}, y = ..density.., fill = diagnosis), alpha = 0.5, bins = 40
    ) +
    geom_density(aes({{ var }}, col = diagnosis), lwd = 2) +
    labs(title = paste0("Distribution of ", label), x = label)
}

plot_hist_den(cancer_data, area_worst, "Area worst")
plot_hist_den(cancer_data, fractal_dimension_mean, "Fractal dimension mean")
plot_hist_den(cancer_data, radius_se, "Radius se")
```

## Correlation & heatmaps
**Heatmaps** provide an informative way to depict two-dimensional data of the kind we have before us. A *heatmap* is an image in which the colour of each pixel is determined by the corresponding value in the array of data.

```{r plot-corr, fig.cap = "Correlation matrix"}
corr_matrix <- cor(input_data)
corr_matrix[!lower.tri(corr_matrix)] <- as.numeric(NA)
corr_df <- corr_matrix |> reshape2::melt() |> as_tibble() |> filter(!is.na(value))
corr_df |> 
  ggplot() +
  geom_tile(aes(Var2, Var1, fill = value)) +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "grey100", 
    midpoint = 0.5, space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  geom_text(aes(Var2, Var1, label = round(value, 2)), color = "black", size = 0.9) +
  theme_void() +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(hjust = 1, size = 8)
  ) +
  coord_fixed() +
  scale_y_discrete(limits = rev(levels(corr_df$Var1)))
```

## Boxplots
Another way of visualising the distribution of the three variables above, grouped by the tumuor type, is the box plots:

```{r plot_boxplot, out.width = "80%"}
cancer_data |>
  ggplot() +
  geom_boxplot(aes(diagnosis, area_worst)) +
  labs(x = "Area worst")
cancer_data |>
  ggplot() +
  geom_boxplot(aes(diagnosis, fractal_dimension_mean)) +
  labs(x = "Fractal dimension mean")
cancer_data |>
  ggplot() +
  geom_boxplot(aes(diagnosis, radius_se)) +
  labs(x = "Radius se")
```

## Smoothed plots
We suspect each of these three variables has an influence on the tumuor status, so we plot `diagnosis` against them with the fitted line according to logistic regression. Note the transformation of `diagnosis` to a probability, with `B` and `M` mapped to 0 and 1, respectively.

```{r plot_smoothed, warning = FALSE, message = FALSE, out.width = "90%"}
plot_logistic_smoothed <- function(df = cancer_data, var) {
  cancer_data |>
    ggplot(aes({{ var }}, as.numeric(diagnosis) - 1.0)) +
    geom_point() +
    geom_smooth(method = "glm", method.args = list(family = "binomial")) +
    labs(y = "Probability")
}
plot_logistic_smoothed(var = area_worst)
plot_logistic_smoothed(var = fractal_dimension_mean)
plot_logistic_smoothed(var = radius_se)
```





# Statistical tests & numerical measures
Next, we perform a two-sample `t`-test, without consider its validity (yet):
```{r ttest, results = "hold"}
area_worst_B <- cancer_data$area_worst[cancer_data$diagnosis == "B"]
area_worst_M <- cancer_data$area_worst[cancer_data$diagnosis == "M"]
ttest0 <- t.test(area_worst_B, area_worst_M, var.equal = TRUE)
paste0("The t-statistic: ", ttest0$statistic)
paste0("The p-value: ", ttest0$p.value, 6)
```

We can write a function to carry out the test more systematically:
```{r ttest_functional}
ttest_var <- function(df = cancer_data, var) {
  vec_B <- df |> filter(diagnosis == "B") |> select({{ var }}) |> unlist()
  vec_M <- df |> filter(diagnosis == "M") |> select({{ var }}) |> unlist()
  ttest0 <- t.test(vec_B, vec_M, var.equal = TRUE)
  print(paste0("Variable name: ", deparse(substitute(var))))
  print(paste0("The t-statistic: ", ttest0$statistic))
  print(paste0("The p-value: ", ttest0$p.value))
}
ttest_var(var = area_worst)
ttest_var(var = fractal_dimension_mean)
ttest_var(var = radius_se)
```

From Figure \@ref(fig:plot-corr), there are pairs of covariates / features that have very high correlation. Including them in a statistical / machine learning model will likely bring about multicollinearity issues. We shall therefore remove them, and print the names of the remaining columns.

```{r drop_corr}
corr_matrix <- cor(input_data) # overwriting previously defined corr_matrix
corr_matrix[!upper.tri(corr_matrix)] <- as.numeric(NA)
corr_threshold <- 0.95
features_to_omit <- unique(which(corr_matrix > corr_threshold, arr.ind = TRUE)[, "col"])
correlation_data <- input_data |> select(-all_of(features_to_omit))
names(correlation_data)
```





# Machine learning

## R equivalent of `train_test_split()` in `sklearn.model_selection`

```{r cross_validation, warning = FALSE}
set.seed(1234)
df_index <-
  createDataPartition(
    cancer_data$diagnosis,
    times = 1,
    p = 0.80,
    list = FALSE
  )
cancer_train <- cancer_data[df_index, ]
cancer_test <- cancer_data[-df_index, ]
tr_control <-
  trainControl(
    method = "cv",
    number = 15,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
logRegress <-
  train(
    diagnosis ~ .,
    data = cancer_data,
    method = 'glm',
    metric = 'ROC',
    preProcess = c('scale', 'center'),
    trControl = tr_control,
    maxit = 100
  )
pred_y <- predict(logRegress, select(cancer_test, -diagnosis))
```

## Confusion matrix

```{r confusion_matrix}
cm <- confusionMatrix(pred_y, cancer_test$diagnosis, positive = 'M')
cm
TN <- cm$table[1,1]
TP <- cm$table[2,2]
FN <- cm$table[2,1]
FP <- cm$table[1,2]
paste0("Accuracy on the test data: ", (TP + TN) / (TP + TN + FN + FP))
```

## Classification report
Classification report is used in machine learning to compute accuracy of a classification model from the values of the confusion matrix. In the classification report, precision is a measure of positive predictions. A few metrics are presented here:

```{r classification_report}
sensitivity(pred_y, cancer_test$diagnosis, positive = "M")
specificity(pred_y, cancer_test$diagnosis, positive = "M")
posPredValue(pred_y, cancer_test$diagnosis, positive = "M")
negPredValue(pred_y, cancer_test$diagnosis, positive = "M")
precision(pred_y, cancer_test$diagnosis, positive = "M")
recall(pred_y, cancer_test$diagnosis, positive = "M")
```

## ROC curve
We first calculate AUC score for the logistic regression model:

```{r auc}
pred_y_num <- as.numeric(pred_y)
test_diagnosis_num <- as.numeric(cancer_test$diagnosis)
auc_cancer <- auc(pred_y_num, test_diagnosis_num)
```

Then we calculate and plot the ROC curve:

```{r roc}
pred_roc <- prediction(pred_y_num, test_diagnosis_num)
roc_perf <- performance(pred_roc, measure = "tpr", x.measure = "fpr")
roc_obj <- roc(pred_y_num, test_diagnosis_num)
ci_auc <- ci.auc(roc_obj)
ci_lower <- round(ci_auc[1], 2)
ci_upper <- round(ci_auc[3], 2)
label_annotate <-
  glue::glue("AUC = {round(auc_cancer, 2)} with 95% CI = ({ci_lower} - {ci_upper})")

ggroc(roc_obj, colour = "lightblue", size = 2, legacy.axes = TRUE) +
  geom_segment(
    aes(x = 0, xend = 1, y = 0, yend = 1),
    colour = "grey",
    linetype = "dashed"
  ) +
  annotate("text", x = 0.7, y = 0.05, label = label_annotate) +
  labs(
    title = "ROC Curve",
    subtitle = paste0("(AUC = ", auc_cancer |> round(4), ")"),
    x = "False Positive Rate",
    y = "True Positive Rate",
    caption = "Source: Wisconsin Breast Cancer Dataset"
  ) +
  theme_minimal()
```




# For reproducibility (via containers?)
For reproducibility purposes, the packages used are shown below:

```{r session_info}
sessionInfo()
```